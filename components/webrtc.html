<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      margin: 0;
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif;
      background: #fafafa;
    }

    .bar {
      padding: 12px 16px;
      display: flex;
      gap: 12px;
      align-items: center;
      border-bottom: 1px solid #ddd;
      background: white;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }

    .bar button {
      padding: 8px 16px;
      border: 1px solid #ddd;
      border-radius: 6px;
      background: white;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.2s;
    }

    .bar button:hover {
      background: #f5f5f5;
      border-color: #999;
    }

    .bar button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .bar select {
      padding: 8px 12px;
      border: 1px solid #ddd;
      border-radius: 6px;
      background: white;
      font-size: 14px;
      min-width: 200px;
    }

    #viewer {
      height: calc(100vh - 80px);
      padding: 16px;
      overflow: auto;
      resize: both;
      border-top: 1px solid #eee;
      background: #fff;
      margin: 8px;
      border-radius: 8px;
      box-shadow: inset 0 1px 3px rgba(0,0,0,0.1);
      min-height: 200px;
      max-height: 80vh;
      overflow-anchor: auto;
      scroll-behavior: smooth;
    }

    .caption-line {
      margin-bottom: 8px;
      line-height: 1.8;
      font-size: 18px;
      font-family: 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', sans-serif;
      padding: 8px 12px;
      border-radius: 6px;
      transition: all 0.3s ease;
      word-break: keep-all;
      overflow-wrap: break-word;
    }

    .unstable {
      color: #666;
      font-style: italic;
      opacity: 0.7;
      border-left: 4px solid #ff9800;
      background: linear-gradient(90deg, #fff8e1 0%, #ffffff 100%);
      box-shadow: 0 1px 3px rgba(255, 152, 0, 0.1);
      animation: pulse 1.5s ease-in-out infinite alternate;
    }

    .stable {
      color: #222;
      font-weight: 500;
      font-style: normal;
      opacity: 1;
      border-left: 4px solid #4CAF50;
      background: linear-gradient(90deg, #e8f5e8 0%, #ffffff 100%);
      box-shadow: 0 2px 4px rgba(76, 175, 80, 0.1);
    }

    @keyframes pulse {
      0% { opacity: 0.7; }
      100% { opacity: 0.9; }
    }

    .badge {
      font-size: 12px;
      color: #666;
      padding: 4px 8px;
      background: #f0f0f0;
      border-radius: 12px;
      border: 1px solid #ddd;
    }

    .badge.connected { background: #e8f5e8; color: #2e7d32; border-color: #4caf50; }
    .badge.connecting { background: #fff3e0; color: #f57c00; border-color: #ff9800; }
    .badge.error { background: #ffebee; color: #c62828; border-color: #f44336; }

    .empty-state {
      text-align: center;
      color: #666;
      margin-top: 100px;
      font-size: 18px;
    }

    .empty-state .icon {
      font-size: 48px;
      margin-bottom: 16px;
      opacity: 0.5;
    }

    /* í•„ìš” ì‹œ overflow-anchor ì œì–´ */
    #viewer {
      overflow-anchor: auto;
    }
  </style>
</head>
<body>
  <div class="bar">
    <button id="btnPerm">ğŸ¤ ê¶Œí•œìš”ì²­</button>
    <select id="selMic" disabled>
      <option value="">ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”</option>
    </select>
    <span id="status" class="badge">idle</span>
    <div style="margin-left: auto; font-size: 12px; color: #666;">
      <span id="latency"></span>
    </div>
  </div>

  <div id="viewer">
    <div class="empty-state">
      <div class="icon">ğŸ™ï¸</div>
      <div>ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”</div>
      <div style="font-size: 14px; margin-top: 8px; color: #999;">
        ì˜ì–´ë¡œ ë§í•˜ë©´ í•œêµ­ì–´ ìë§‰ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤
      </div>
    </div>
  </div>

<script>
// Bootstrap data from Streamlit
const BOOT = {{BOOTSTRAP_JSON}};

// Global state
let pc = null;
let dc = null;
let localStream = null;
let currentDeviceId = null;
let startTime = null;
let currentUnstableLine = null;  // í˜„ì¬ ì§„í–‰ ì¤‘ì¸ unstable ë¼ì¸
let responseInProgress = false;  // ì‘ë‹µ ìƒì„± ì¤‘ì¸ì§€ ì¶”ì 
let pendingTranscript = null;  // ë²ˆì—­ ëŒ€ê¸° ì¤‘ì¸ ì›ë¬¸
let pendingTranslation = '';  // ëˆ„ì  ì¤‘ì¸ ë²ˆì—­

// DOM elements
const viewer = document.getElementById('viewer');
const selMic = document.getElementById('selMic');
const statusEl = document.getElementById('status');
const latencyEl = document.getElementById('latency');
const btnPerm = document.getElementById('btnPerm');

// Utility functions
function logStatus(status, className = '') {
  statusEl.textContent = status;
  statusEl.className = `badge ${className}`;
  console.log(`[Status] ${status}`);
}

function logLatency(ms) {
  if (ms > 0) {
    latencyEl.textContent = `${ms}ms`;
  } else {
    latencyEl.textContent = '';
  }
}

// Permission and device handling
async function ensurePermission() {
  try {
    logStatus('ê¶Œí•œ ìš”ì²­ ì¤‘...', 'connecting');

    // Check if permissions are already granted
    const permissionStatus = await navigator.permissions.query({name: 'microphone'});
    console.log('[Permission] Current status:', permissionStatus.state);

    if (permissionStatus.state === 'denied') {
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”.');
    }

    // Request microphone access
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false,
        sampleRate: 24000
      }
    });

    // Stop the temporary stream
    stream.getTracks().forEach(track => track.stop());

    logStatus('ê¶Œí•œ í—ˆìš©ë¨', 'connected');
    console.log('[Permission] Microphone access granted');
    return true;
  } catch (error) {
    logStatus('ê¶Œí•œ ê±°ë¶€ë¨', 'error');
    console.error('[Permission] Access denied:', error);

    // Show more specific error message
    if (error.name === 'NotAllowedError') {
      console.error('[Permission] User denied microphone access');
    } else if (error.name === 'NotFoundError') {
      console.error('[Permission] No microphone found');
    } else if (error.name === 'NotReadableError') {
      console.error('[Permission] Microphone is being used by another application');
    }

    return false;
  }
}

async function listMics() {
  try {
    console.log('[Devices] Enumerating audio input devices...');
    const devices = await navigator.mediaDevices.enumerateDevices();
    const audioInputs = devices.filter(d => d.kind === 'audioinput');

    selMic.innerHTML = '<option value="">ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”</option>';

    if (audioInputs.length === 0) {
      const option = document.createElement('option');
      option.value = '';
      option.textContent = 'ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
      option.disabled = true;
      selMic.appendChild(option);
      console.warn('[Devices] No audio input devices found');
      return;
    }

    audioInputs.forEach((device, index) => {
      const option = document.createElement('option');
      option.value = device.deviceId;

      // Use device label if available, otherwise create a descriptive name
      let deviceLabel = device.label;
      if (!deviceLabel || deviceLabel.trim() === '') {
        deviceLabel = `ë§ˆì´í¬ ${index + 1}`;
        if (device.deviceId === 'default') {
          deviceLabel = 'ê¸°ë³¸ ë§ˆì´í¬';
        }
      }

      option.textContent = deviceLabel;
      selMic.appendChild(option);

      console.log(`[Devices] Found: ${deviceLabel} (${device.deviceId.substring(0, 20)}...)`);
    });

    // Auto-select default device if no device is currently selected
    if (!currentDeviceId && audioInputs.length > 0) {
      const defaultDevice = audioInputs.find(d => d.deviceId === 'default') || audioInputs[0];
      currentDeviceId = defaultDevice.deviceId;
      selMic.value = currentDeviceId;
      console.log(`[Devices] Auto-selected: ${defaultDevice.label || 'default device'}`);
    } else if (currentDeviceId) {
      selMic.value = currentDeviceId;
    }

    selMic.disabled = false;
    console.log(`[Devices] Successfully enumerated ${audioInputs.length} audio input devices`);
  } catch (error) {
    console.error('[Devices] Failed to enumerate devices:', error);
    selMic.innerHTML = '<option value="">ì¥ì¹˜ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</option>';
    selMic.disabled = true;
  }
}

async function getStream(deviceId) {
  try {
    console.log('[Stream] Acquiring audio stream...');

    // Stop previous stream
    if (localStream) {
      console.log('[Stream] Stopping previous stream');
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log(`[Stream] Stopped track: ${track.kind} - ${track.label}`);
      });
      localStream = null;
    }

    // Determine device to use
    const targetDeviceId = deviceId || selMic.value || currentDeviceId;
    console.log(`[Stream] Target device: ${targetDeviceId || 'default'}`);

    const constraints = {
      audio: {
        deviceId: targetDeviceId ? { exact: targetDeviceId } : undefined,
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false,
        sampleRate: 24000,  // OpenAI Realtime optimal sample rate
        channelCount: 1,    // Mono audio
        latency: 0.01       // Low latency for real-time
      }
    };

    console.log('[Stream] Requesting stream with constraints:', constraints);
    localStream = await navigator.mediaDevices.getUserMedia(constraints);

    // Update current device
    currentDeviceId = targetDeviceId;

    // Log stream info
    const audioTrack = localStream.getAudioTracks()[0];
    if (audioTrack) {
      const settings = audioTrack.getSettings();
      console.log('[Stream] Audio track acquired:', {
        label: audioTrack.label,
        deviceId: settings.deviceId,
        sampleRate: settings.sampleRate,
        channelCount: settings.channelCount,
        echoCancellation: settings.echoCancellation,
        noiseSuppression: settings.noiseSuppression,
        autoGainControl: settings.autoGainControl
      });
    }

    return localStream;
  } catch (error) {
    console.error('[Stream] Failed to get audio stream:', error);

    // Provide specific error messages
    if (error.name === 'NotFoundError') {
      throw new Error(`ì§€ì •ëœ ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${deviceId || 'default'}`);
    } else if (error.name === 'NotAllowedError') {
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (error.name === 'NotReadableError') {
      throw new Error('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.');
    } else if (error.name === 'OverconstrainedError') {
      throw new Error('ìš”ì²­ëœ ì˜¤ë””ì˜¤ ì„¤ì •ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤.');
    }

    throw error;
  }
}

// Caption viewer functions
function appendLine(text, className = 'stable') {
  if (!text || !text.trim()) return;

  // Remove empty state if present
  const emptyState = viewer.querySelector('.empty-state');
  if (emptyState) {
    emptyState.remove();
  }

  // Check if we're at the bottom for auto-scroll with more tolerance
  const atBottom = Math.abs(viewer.scrollHeight - viewer.scrollTop - viewer.clientHeight) < 10;

  if (className === 'unstable') {
    // For unstable text, update the current line or create new one
    if (!currentUnstableLine) {
      currentUnstableLine = document.createElement('div');
      currentUnstableLine.className = `caption-line ${className}`;
      currentUnstableLine.textContent = '';
      viewer.appendChild(currentUnstableLine);
    }
    // Add space before new text if line already has content
    const needsSpace = currentUnstableLine.textContent.length > 0 && !text.startsWith(' ');
    currentUnstableLine.textContent += (needsSpace ? ' ' : '') + text.trim();
  } else {
    // For stable text, replace unstable line or create new stable line
    if (currentUnstableLine) {
      // Replace the unstable line with stable content
      currentUnstableLine.className = 'caption-line stable';
      currentUnstableLine.textContent = text.trim();
      currentUnstableLine = null;
    } else {
      // Create new stable line
      const div = document.createElement('div');
      div.className = `caption-line ${className}`;
      div.textContent = text.trim();
      viewer.appendChild(div);
    }
  }

  // Auto-scroll only if user was at bottom with smooth animation
  if (atBottom) {
    requestAnimationFrame(() => {
      viewer.scrollTo({
        top: viewer.scrollHeight,
        behavior: 'smooth'
      });
    });
  }

  console.log(`[Caption] ${className}: ${text.trim()}`);
}

function clearViewer() {
  viewer.innerHTML = `
    <div class="empty-state">
      <div class="icon">ğŸ™ï¸</div>
      <div>ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”</div>
      <div style="font-size: 14px; margin-top: 8px; color: #999;">
        ì˜ì–´ë¡œ ë§í•˜ë©´ í•œêµ­ì–´ ìë§‰ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤
      </div>
    </div>
  `;
  currentUnstableLine = null;  // unstable ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
  console.log('[Caption] Viewer cleared');
}

// ì›ë¬¸ê³¼ ë²ˆì—­ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
function displayTranscriptAndTranslation(transcript, translation) {
  // ì›ë¬¸ ë¨¼ì € í‘œì‹œ
  appendLine(`[ì›ë¬¸] ${transcript}`, 'stable');

  // ë²ˆì—­ í‘œì‹œ
  if (translation && translation !== '[ë²ˆì—­ ì—†ìŒ]' && translation !== '[ë²ˆì—­ ì‹¤íŒ¨]' && translation !== '[ë²ˆì—­ ë¶ˆê°€]') {
    appendLine(translation, 'stable');
  } else {
    appendLine(translation, 'stable');
  }
}

// WebRTC connection handling
async function connectRealtime(ephemeral, model) {
  try {
    logStatus('ì—°ê²° ì¤‘...', 'connecting');
    startTime = Date.now();

    // Validate inputs
    if (!ephemeral?.client_secret?.value) {
      throw new Error('ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì…ë‹ˆë‹¤.');
    }

    console.log('[WebRTC] Starting connection with model:', model);

    // Create RTCPeerConnection with comprehensive configuration
    pc = new RTCPeerConnection({
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
      ],
      iceCandidatePoolSize: 10,
      bundlePolicy: 'max-bundle',
      rtcpMuxPolicy: 'require'
    });

    // Monitor connection state changes
    pc.onconnectionstatechange = () => {
      const state = pc.connectionState;
      console.log('[WebRTC] Connection state changed:', state);

      switch (state) {
        case 'connecting':
          logStatus('ì—°ê²° ì¤‘...', 'connecting');
          break;
        case 'connected':
          if (!dc || dc.readyState !== 'open') {
            logStatus('ë°ì´í„° ì±„ë„ ëŒ€ê¸° ì¤‘...', 'connecting');
          }
          break;
        case 'disconnected':
          logStatus('ì—°ê²° ëŠê¹€', 'error');
          break;
        case 'failed':
          logStatus('ì—°ê²° ì‹¤íŒ¨', 'error');
          console.error('[WebRTC] Connection failed - cleaning up');
          closeConnection();
          break;
        case 'closed':
          logStatus('ì—°ê²° ì¢…ë£Œë¨', '');
          break;
      }
    };

    // Monitor ICE connection state
    pc.oniceconnectionstatechange = () => {
      console.log('[WebRTC] ICE connection state:', pc.iceConnectionState);
      if (pc.iceConnectionState === 'failed') {
        console.error('[WebRTC] ICE connection failed');
        logStatus('ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨', 'error');
      }
    };

    // Log ICE candidates for debugging
    pc.onicecandidate = (event) => {
      if (event.candidate) {
        console.log('[WebRTC] ICE candidate:', event.candidate.type, event.candidate.candidate.substring(0, 50) + '...');
      } else {
        console.log('[WebRTC] ICE gathering completed');
      }
    };

    // Create data channel for OpenAI events
    dc = pc.createDataChannel('oai-events', {
      ordered: true,
      maxRetransmits: 0  // Real-time priority over reliability
    });

    dc.onopen = () => {
      logStatus('ì—°ê²°ë¨', 'connected');
      console.log('[WebRTC] Data channel opened successfully');

      // Send session configuration with auto-response enabled
      const sessionUpdate = {
        type: 'session.update',
        session: {
          instructions: 'You are a translation-only bot. ONLY translate English to Korean. Rules: 1) Translate naturally avoiding awkward passive voice 2) Use active voice when appropriate 3) Make it sound natural in Korean 4) No quotes, greetings, or explanations 5) Just output the Korean translation 6) Preserve proper nouns in original language. ALWAYS respond with Korean translation immediately after hearing English.',
          voice: 'alloy',  // ìŒì„±ì€ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í•„ìˆ˜ í•„ë“œ
          input_audio_format: 'pcm16',
          output_audio_format: 'pcm16',  // í•„ìˆ˜ í•„ë“œì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
          input_audio_transcription: {
            model: 'whisper-1',
            language: 'en'
          },
          turn_detection: {
            type: 'server_vad',
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 700,
            create_response: true  // ìë™ ì‘ë‹µ ìƒì„± í™œì„±í™”
          },
          modalities: ['text'],  // í…ìŠ¤íŠ¸ ì¶œë ¥ë§Œ ì‚¬ìš©
          temperature: 0.6,  // ìµœì†Œ í—ˆìš©ê°’ìœ¼ë¡œ ì¼ê´€ëœ ë²ˆì—­ ì‹œë„
          max_response_output_tokens: 150
        }
      };

      try {
        dc.send(JSON.stringify(sessionUpdate));
        console.log('[WebRTC] Session configuration sent:', sessionUpdate);
      } catch (error) {
        console.error('[WebRTC] Failed to send session configuration:', error);
        logStatus('ì„¤ì • ì „ì†¡ ì‹¤íŒ¨', 'error');
      }
    };

    dc.onmessage = (event) => {
      try {
        const message = JSON.parse(event.data);
        handleRealtimeMessage(message);
      } catch (error) {
        console.error('[WebRTC] Failed to parse message:', error, 'Raw data:', event.data);
      }
    };

    dc.onerror = (error) => {
      console.error('[WebRTC] Data channel error:', error);
      logStatus('ë°ì´í„° ì±„ë„ ì˜¤ë¥˜', 'error');
    };

    dc.onclose = () => {
      console.log('[WebRTC] Data channel closed');
      if (pc && pc.connectionState !== 'closed') {
        logStatus('ë°ì´í„° ì±„ë„ ì¢…ë£Œë¨', 'error');
      }
    };

    // Add audio track with proper handling
    console.log('[WebRTC] Getting audio stream...');
    const stream = await getStream(currentDeviceId);

    if (!stream || stream.getAudioTracks().length === 0) {
      throw new Error('ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    stream.getTracks().forEach(track => {
      const sender = pc.addTrack(track, stream);
      console.log('[WebRTC] Audio track added:', track.label, 'enabled:', track.enabled);

      // Monitor track state
      track.onended = () => {
        console.warn('[WebRTC] Audio track ended unexpectedly');
        logStatus('ì˜¤ë””ì˜¤ ì—°ê²° ëŠê¹€', 'error');
      };
    });

    // Create offer and set local description
    console.log('[WebRTC] Creating SDP offer...');
    const offer = await pc.createOffer({
      offerToReceiveAudio: false,
      offerToReceiveVideo: false
    });

    await pc.setLocalDescription(offer);
    console.log('[WebRTC] Local description set');

    // Wait for ICE gathering to complete or timeout
    await waitForIceGathering(pc, 5000);

    // Send SDP to OpenAI with retry logic
    console.log('[WebRTC] Sending SDP to OpenAI...');
    const url = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`;
    const headers = {
      'Authorization': `Bearer ${ephemeral.client_secret.value}`,
      'Content-Type': 'application/sdp',
      'OpenAI-Beta': 'realtime=v1',
    };

    console.log('[WebRTC] Request URL:', url);
    console.log('[WebRTC] Request headers:', { ...headers, 'Authorization': 'Bearer [REDACTED]' });

    const sdpResponse = await fetch(url, {
      method: 'POST',
      body: offer.sdp,
      headers: headers
    });

    if (!sdpResponse.ok) {
      const errorText = await sdpResponse.text();
      console.error('[WebRTC] SDP exchange failed:', sdpResponse.status, sdpResponse.statusText, errorText);

      if (sdpResponse.status === 401) {
        throw new Error('ì¸ì¦ ì‹¤íŒ¨: í† í°ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      } else if (sdpResponse.status === 429) {
        throw new Error('ìš”ì²­ í•œë„ ì´ˆê³¼: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      } else {
        throw new Error(`SDP êµí™˜ ì‹¤íŒ¨: ${sdpResponse.status} ${sdpResponse.statusText}`);
      }
    }

    const answerSdp = await sdpResponse.text();
    console.log('[WebRTC] Received SDP answer, length:', answerSdp.length);

    const answer = { type: 'answer', sdp: answerSdp };
    await pc.setRemoteDescription(answer);

    console.log('[WebRTC] Remote description set - connection process initiated');

  } catch (error) {
    console.error('[WebRTC] Connection failed:', error);

    // Provide user-friendly error messages
    let userMessage = 'ì—°ê²° ì‹¤íŒ¨';
    if (error.message?.includes('ì¸ì¦')) {
      userMessage = 'ì¸ì¦ ì‹¤íŒ¨';
    } else if (error.message?.includes('í† í°')) {
      userMessage = 'í† í° ì˜¤ë¥˜';
    } else if (error.message?.includes('ë„¤íŠ¸ì›Œí¬')) {
      userMessage = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜';
    } else if (error.message?.includes('ì˜¤ë””ì˜¤')) {
      userMessage = 'ì˜¤ë””ì˜¤ ì˜¤ë¥˜';
    }

    logStatus(userMessage, 'error');

    // Clean up on failure
    closeConnection();
    throw error;
  }
}

// Helper function to wait for ICE gathering completion
function waitForIceGathering(pc, timeout = 5000) {
  return new Promise((resolve) => {
    if (pc.iceGatheringState === 'complete') {
      resolve();
      return;
    }

    const timeoutId = setTimeout(() => {
      console.log('[WebRTC] ICE gathering timeout - proceeding anyway');
      resolve();
    }, timeout);

    pc.addEventListener('icegatheringstatechange', () => {
      if (pc.iceGatheringState === 'complete') {
        console.log('[WebRTC] ICE gathering completed');
        clearTimeout(timeoutId);
        resolve();
      }
    });
  });
}

function handleRealtimeMessage(message) {
  // Record first message latency
  if (startTime && !latencyEl.textContent) {
    const latency = Date.now() - startTime;
    logLatency(latency);
    startTime = null;
  }

  console.log('[Realtime]', message.type, message);

  // Handle different message types from OpenAI Realtime API
  switch (message.type) {
    // Session and connection events
    case 'session.created':
      console.log('[Realtime] Session created:', message.session?.id);
      break;

    case 'session.updated':
      console.log('[Realtime] Session updated');
      break;

    // Input audio transcription events (ì›ë¬¸ í‘œì‹œ)
    case 'conversation.item.input_audio_transcription.delta':
      // ì›ë¬¸ì€ delta(unstable) ìƒíƒœë¥¼ í‘œì‹œí•˜ì§€ ì•ŠìŒ
      console.log('[Realtime] Transcription delta:', message.delta);
      break;

    case 'conversation.item.input_audio_transcription.completed':
      if (message.transcript) {
        // ì´ì „ ë²ˆì—­ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê°•ì œë¡œ ì™„ë£Œ ì²˜ë¦¬
        if (pendingTranscript) {
          displayTranscriptAndTranslation(pendingTranscript, pendingTranslation || '[ë²ˆì—­ ì—†ìŒ]');
        }

        // ìƒˆë¡œìš´ ì›ë¬¸ ì„¤ì •
        pendingTranscript = message.transcript;
        pendingTranslation = '';

        console.log('[Transcription] Completed:', message.transcript);

        // ë²ˆì—­ ìš”ì²­ (ì‘ë‹µ ì§„í–‰ ì¤‘ì¼ ë•ŒëŠ” ëŒ€ê¸°)
        if (dc && dc.readyState === 'open' && !responseInProgress) {
          responseInProgress = true;
          const responseCreateRequest = {
            type: 'response.create'
          };

          try {
            dc.send(JSON.stringify(responseCreateRequest));
            console.log('[Translation] Requested translation for:', message.transcript);

            // íƒ€ì„ì•„ì›ƒ ì„¤ì • (10ì´ˆ í›„ ê°•ì œ í•´ì œ, í‘œì‹œí•˜ì§€ ì•ŠìŒ)
            setTimeout(() => {
              if (responseInProgress && pendingTranscript === message.transcript) {
                console.warn('[Translation] Timeout - releasing response lock but keeping transcript');
                responseInProgress = false;
                // íƒ€ì„ì•„ì›ƒì´ì–´ë„ ë²ˆì—­ì´ ë‚˜ì¤‘ì— ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í‘œì‹œí•˜ì§€ ì•ŠìŒ
              }
            }, 10000);

          } catch (error) {
            console.error('[Translation] Failed to request translation:', error);
            responseInProgress = false;
            // ë²ˆì—­ ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ë§Œ í‘œì‹œ
            displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ì‹¤íŒ¨]');
            pendingTranscript = null;
          }
        } else if (responseInProgress) {
          console.log('[Translation] Response already in progress, will wait for next opportunity');
          // ì‘ë‹µì´ ì§„í–‰ ì¤‘ì´ë©´ í˜„ì¬ transcriptëŠ” ìœ ì§€í•˜ê³  ë‹¤ìŒ ê¸°íšŒë¥¼ ê¸°ë‹¤ë¦¼
        } else {
          // ì—°ê²° ìƒíƒœê°€ ì¢‹ì§€ ì•Šì€ ê²½ìš°
          console.warn('[Translation] DataChannel not ready, state:', dc?.readyState);
          displayTranscriptAndTranslation(pendingTranscript, '[ì—°ê²° ë¶ˆì•ˆì •]');
          pendingTranscript = null;
        }
      }
      break;

    case 'conversation.item.input_audio_transcription.failed':
      console.error('[Realtime] Transcription failed:', message.error);
      appendLine('[ìŒì„±ì¸ì‹ ì‹¤íŒ¨]', 'stable');
      break;

    // Translation output events (ë²ˆì—­ ê²°ê³¼)
    case 'response.text.delta':
      if (message.delta && pendingTranscript) {
        // ë”°ì˜´í‘œ ì œê±°
        const cleanDelta = message.delta.replace(/^["']|["']$/g, '');

        // ëŒ€í™” ì‹œë„í•˜ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§
        if (cleanDelta.toLowerCase().includes('here to assist') ||
            cleanDelta.toLowerCase().includes('how may i help') ||
            cleanDelta.toLowerCase().includes('how can i help') ||
            cleanDelta.toLowerCase().includes('language learning') ||
            cleanDelta.toLowerCase().includes('translation') && cleanDelta.toLowerCase().includes('assist')) {
          console.log('[Delta] Filtered out conversation attempt:', cleanDelta);
          return;
        }

        if (cleanDelta.trim()) {
          pendingTranslation += cleanDelta;
          // ì‹¤ì‹œê°„ìœ¼ë¡œ ë²ˆì—­ ì§„í–‰ ìƒí™© í‘œì‹œí•˜ì§€ ì•ŠìŒ (ìˆœì„œ ë³´ì¥ì„ ìœ„í•´)
        }
      }
      break;

    case 'response.text.done':
      if (message.text && pendingTranscript) {
        // ë”°ì˜´í‘œ ì œê±° ë° ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ í•„í„°ë§
        let cleanText = message.text.replace(/^["']|["']$/g, '').trim();

        // ëŒ€í™” ì‹œë„í•˜ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ í•„í„°ë§
        const conversationPatterns = [
          'here to assist', 'how may i help', 'how can i help', 'language learning',
          'further assistance', 'feel free to ask', 'any more questions',
          'other inquiries', 'anything else', 'any other', 'i\'m here',
          'assistance', 'inquiries'
        ];

        const lowerText = cleanText.toLowerCase();
        const isConversation = conversationPatterns.some(pattern => lowerText.includes(pattern));

        if (isConversation) {
          console.log('[Realtime] Filtered out conversation attempt:', cleanText);
          cleanText = '';
        }

        // ìµœì¢… ë²ˆì—­ ê²°ê³¼ ì‚¬ìš© (done í…ìŠ¤íŠ¸ ìš°ì„ , delta ëˆ„ì ì€ ë³´ì¡°)
        const deltaTranslation = pendingTranslation.trim();
        const doneTranslation = cleanText;

        // done í…ìŠ¤íŠ¸ê°€ ë” ì™„ì „í•œ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ìš°ì„  ì‚¬ìš©
        let finalTranslation = doneTranslation || deltaTranslation;

        // ë‘ ë²ˆì—­ì´ ëª¨ë‘ ìˆê³  doneì´ deltaë³´ë‹¤ ê¸´ ê²½ìš° done ì‚¬ìš©
        if (doneTranslation && deltaTranslation && doneTranslation.length > deltaTranslation.length) {
          finalTranslation = doneTranslation;
        }

        console.log('[Translation Debug] Delta:', deltaTranslation, 'Done:', doneTranslation, 'Final:', finalTranslation);

        if (finalTranslation && finalTranslation.length >= 1) {
          // ì›ë¬¸ê³¼ ë²ˆì—­ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
          displayTranscriptAndTranslation(pendingTranscript, finalTranslation);
          console.log('[Translation] Completed:', finalTranslation);
        } else {
          // ë²ˆì—­ì´ ì—†ëŠ” ê²½ìš° ì›ë¬¸ë§Œ í‘œì‹œ
          displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ì—†ìŒ]');
          console.log('[Translation] No translation received for:', pendingTranscript);
        }

        // ìƒíƒœ ì´ˆê¸°í™”
        pendingTranscript = null;
        pendingTranslation = '';
      }
      break;

    // Response generation tracking
    case 'response.created':
      console.log('[Realtime] Response generation started');
      responseInProgress = true;
      break;

    case 'response.done':
      console.log('[Realtime] Response generation completed');
      responseInProgress = false;

      // ë²ˆì—­ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë„ ê¸°ë‹¤ë¦¼ (ì´í›„ì— ë²ˆì—­ì´ ì˜¬ ìˆ˜ ìˆìŒ)
      if (pendingTranscript && !pendingTranslation.trim()) {
        console.log('[Translation] Response done but waiting for translation...');
        // ë²ˆì—­ ëˆ„ë½ í‘œì‹œí•˜ì§€ ì•Šê³  ê¸°ë‹¤ë¦¼

        // ì‘ë‹µì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ìƒˆë¡œìš´ ë²ˆì—­ ìš”ì²­ ì‹œë„
        if (dc && dc.readyState === 'open') {
          console.log('[Translation] Retrying translation request after response done');
          responseInProgress = true;
          const retryRequest = {
            type: 'response.create'
          };

          try {
            dc.send(JSON.stringify(retryRequest));
          } catch (error) {
            console.error('[Translation] Retry request failed:', error);
            responseInProgress = false;
          }
        }
      }
      break;

    case 'response.output_item.added':
    case 'response.content_part.added':
    case 'response.output_item.done':
      console.log('[Realtime] Translation event:', message.type);
      break;

    // Audio events (if enabled)
    case 'response.audio.delta':
      // We're not using audio output, so just log
      console.log('[Realtime] Audio delta received (ignored)');
      break;

    case 'response.audio.done':
      console.log('[Realtime] Audio generation completed (ignored)');
      break;

    // Turn detection events
    case 'input_audio_buffer.speech_started':
      console.log('[Realtime] Speech started');
      logStatus('ë§í•˜ëŠ” ì¤‘...', 'connecting');
      break;

    case 'input_audio_buffer.speech_stopped':
      console.log('[Realtime] Speech stopped');
      logStatus('ì²˜ë¦¬ ì¤‘...', 'connecting');
      break;

    case 'input_audio_buffer.committed':
      console.log('[Realtime] Audio buffer committed');
      break;

    case 'input_audio_buffer.cleared':
      console.log('[Realtime] Audio buffer cleared');
      break;

    // Conversation events
    case 'conversation.item.created':
      console.log('[Realtime] Conversation item created:', message.item?.type);
      break;

    case 'conversation.item.truncated':
      console.log('[Realtime] Conversation item truncated');
      break;

    case 'conversation.item.deleted':
      console.log('[Realtime] Conversation item deleted');
      break;

    // Rate limiting events
    case 'rate_limits.updated':
      console.log('[Realtime] Rate limits updated:', message.rate_limits);
      break;

    // Error handling
    case 'error':
      console.error('[Realtime] API Error:', message.error);
      const errorCode = message.error?.code;
      const errorMessage = message.error?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';

      if (errorCode === 'invalid_api_key') {
        logStatus('ì¸ì¦ ì˜¤ë¥˜', 'error');
        appendLine('[ì˜¤ë¥˜] ì¸ì¦ í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.', 'stable');
      } else if (errorCode === 'rate_limit_exceeded') {
        logStatus('ì‚¬ìš©ëŸ‰ í•œë„ ì´ˆê³¼', 'error');
        appendLine('[ì˜¤ë¥˜] ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'stable');
      } else if (errorCode === 'insufficient_quota') {
        logStatus('í• ë‹¹ëŸ‰ ë¶€ì¡±', 'error');
        appendLine('[ì˜¤ë¥˜] ê³„ì • í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.', 'stable');
      } else {
        logStatus('API ì˜¤ë¥˜', 'error');
        appendLine(`[ì˜¤ë¥˜] ${errorMessage}`, 'stable');
      }
      break;

    // Connection and server events
    case 'server.heartbeat':
      // Optional: log heartbeat for debugging
      console.debug('[Realtime] Server heartbeat');
      break;

    default:
      console.log('[Realtime] Unhandled message type:', message.type, message);

      // Try to extract any text content generically
      const textContent = message.delta || message.text || message.transcript;
      if (textContent && typeof textContent === 'string' && textContent.trim()) {
        const isUnstable = message.type?.includes('delta') || message.type?.includes('partial');
        appendLine(textContent, isUnstable ? 'unstable' : 'stable');
      }
      break;
  }

  // Reset connection status to connected if we were processing
  if (statusEl.textContent === 'ì²˜ë¦¬ ì¤‘...' && message.type?.includes('done')) {
    logStatus('ì—°ê²°ë¨', 'connected');
  }
}

function closeConnection() {
  try {
    if (dc) {
      dc.close();
      dc = null;
    }

    if (pc) {
      pc.close();
      pc = null;
    }

    if (localStream) {
      localStream.getTracks().forEach(track => track.stop());
      localStream = null;
    }

    currentUnstableLine = null;  // unstable ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
    responseInProgress = false;  // ì‘ë‹µ ìƒíƒœ ì´ˆê¸°í™”
    pendingTranscript = null;  // ëŒ€ê¸° ì¤‘ì¸ ì›ë¬¸ ì´ˆê¸°í™”
    pendingTranslation = '';  // ëˆ„ì  ì¤‘ì¸ ë²ˆì—­ ì´ˆê¸°í™”
    logStatus('idle', '');
    logLatency(0);
    console.log('[WebRTC] Connection closed');
  } catch (error) {
    console.error('[WebRTC] Error closing connection:', error);
  }
}

// Event handlers
btnPerm.onclick = async () => {
  const success = await ensurePermission();
  if (success) {
    await listMics();
  }
};

selMic.onchange = async (event) => {
  const newDeviceId = event.target.value;
  console.log(`[Device] Device selection changed: ${currentDeviceId} -> ${newDeviceId}`);

  if (!newDeviceId) {
    console.log('[Device] No device selected');
    return;
  }

  const wasConnected = pc && (pc.connectionState === 'connected' || pc.connectionState === 'connecting');

  try {
    // Update the audio stream with new device
    currentDeviceId = newDeviceId;

    if (wasConnected) {
      logStatus('ì¥ì¹˜ ë³€ê²½ ì¤‘...', 'connecting');
      console.log('[Device] Switching audio device during active connection');

      // Get new stream with selected device
      const newStream = await getStream(newDeviceId);

      // Replace tracks in the existing peer connection
      const audioTrack = newStream.getAudioTracks()[0];
      const sender = pc.getSenders().find(s => s.track && s.track.kind === 'audio');

      if (sender && audioTrack) {
        await sender.replaceTrack(audioTrack);
        console.log('[Device] Audio track replaced successfully');
        logStatus('ì—°ê²°ë¨', 'connected');
      } else {
        console.warn('[Device] Could not find audio sender to replace track');
        // Fallback: add new track
        pc.addTrack(audioTrack, newStream);
      }
    } else {
      console.log('[Device] Device changed while not connected - will use new device on next connection');
    }

  } catch (error) {
    console.error('[Device] Failed to switch audio device:', error);
    logStatus('ì¥ì¹˜ ë³€ê²½ ì‹¤íŒ¨', 'error');

    // Revert selection if it failed
    if (currentDeviceId) {
      selMic.value = currentDeviceId;
    }
  }
};

// Bootstrap actions based on Streamlit state
(async () => {
  console.log('[Bootstrap] Action:', BOOT.action, BOOT);

  try {
    if (BOOT.action === 'start' && BOOT.ephemeral) {
      // Ensure permission and list devices
      const hasPermission = await ensurePermission();
      if (hasPermission) {
        await listMics();
        // Start WebRTC connection
        await connectRealtime(BOOT.ephemeral, BOOT.model);
      }
    } else if (BOOT.action === 'stop') {
      closeConnection();
      clearViewer();
    } else if (BOOT.action === 'idle') {
      logStatus('ëŒ€ê¸° ì¤‘', '');
    }
  } catch (error) {
    console.error('[Bootstrap] Failed:', error);
    logStatus('ì´ˆê¸°í™” ì‹¤íŒ¨', 'error');
  }
})();

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
  closeConnection();
});

</script>
</body>
</html>
