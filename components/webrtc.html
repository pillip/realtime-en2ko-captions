<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      margin: 0;
      font-family: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif;
      background: #fafafa;
    }

    .bar {
      padding: 12px 16px;
      display: flex;
      gap: 12px;
      align-items: center;
      border-bottom: 1px solid #ddd;
      background: white;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }

    .bar button {
      padding: 8px 16px;
      border: 1px solid #ddd;
      border-radius: 6px;
      background: white;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.2s;
    }

    .bar button:hover {
      background: #f5f5f5;
      border-color: #999;
    }

    .bar button:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .bar select {
      padding: 8px 12px;
      border: 1px solid #ddd;
      border-radius: 6px;
      background: white;
      font-size: 14px;
      min-width: 200px;
    }

    #viewer {
      height: calc(100vh - 80px);
      padding: 16px;
      overflow: auto;
      resize: both;
      border-top: 1px solid #eee;
      background: #fff;
      margin: 8px;
      border-radius: 8px;
      box-shadow: inset 0 1px 3px rgba(0,0,0,0.1);
      min-height: 200px;
      max-height: 80vh;
      overflow-anchor: auto;
      scroll-behavior: smooth;
    }

    .caption-line {
      margin-bottom: 8px;
      line-height: 1.8;
      font-size: 18px;
      font-family: 'Malgun Gothic', 'ë§‘ì€ ê³ ë”•', sans-serif;
      padding: 8px 12px;
      border-radius: 6px;
      transition: all 0.3s ease;
      word-break: keep-all;
      overflow-wrap: break-word;
    }

    .unstable {
      color: #666;
      font-style: italic;
      opacity: 0.7;
      border-left: 4px solid #ff9800;
      background: linear-gradient(90deg, #fff8e1 0%, #ffffff 100%);
      box-shadow: 0 1px 3px rgba(255, 152, 0, 0.1);
      animation: pulse 1.5s ease-in-out infinite alternate;
    }

    .stable {
      color: #222;
      font-weight: 500;
      font-style: normal;
      opacity: 1;
      border-left: 4px solid #4CAF50;
      background: linear-gradient(90deg, #e8f5e8 0%, #ffffff 100%);
      box-shadow: 0 2px 4px rgba(76, 175, 80, 0.1);
    }

    @keyframes pulse {
      0% { opacity: 0.7; }
      100% { opacity: 0.9; }
    }

    .badge {
      font-size: 12px;
      color: #666;
      padding: 4px 8px;
      background: #f0f0f0;
      border-radius: 12px;
      border: 1px solid #ddd;
    }

    .badge.connected { background: #e8f5e8; color: #2e7d32; border-color: #4caf50; }
    .badge.connecting { background: #fff3e0; color: #f57c00; border-color: #ff9800; }
    .badge.error { background: #ffebee; color: #c62828; border-color: #f44336; }

    .empty-state {
      text-align: center;
      color: #666;
      margin-top: 100px;
      font-size: 18px;
    }

    .empty-state .icon {
      font-size: 48px;
      margin-bottom: 16px;
      opacity: 0.5;
    }

    /* í•„ìš” ì‹œ overflow-anchor ì œì–´ */
    #viewer {
      overflow-anchor: auto;
    }
  </style>
</head>
<body>
  <div class="bar">
    <button id="btnPerm">ğŸ¤ ê¶Œí•œìš”ì²­</button>
    <button id="btnPause" style="display: none;">â¸ï¸ ì¼ì‹œì •ì§€</button>
    <select id="selMic" disabled>
      <option value="">ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”</option>
    </select>
    <span id="status" class="badge">idle</span>
    <div style="margin-left: auto; font-size: 12px; color: #666;">
      <span id="latency"></span>
    </div>
  </div>

  <div id="viewer">
    <div class="empty-state">
      <div class="icon">ğŸ™ï¸</div>
      <div>ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”</div>
      <div style="font-size: 14px; margin-top: 8px; color: #999;">
        ì˜ì–´ë¡œ ë§í•˜ë©´ í•œêµ­ì–´ ìë§‰ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤
      </div>
    </div>
  </div>

  <!-- Toast Container -->
  <div id="toastContainer" style="
    position: fixed;
    top: 20px;
    right: 20px;
    z-index: 1000;
    max-width: 400px;
  "></div>

<script>
// Bootstrap data from Streamlit
const BOOT = {{BOOTSTRAP_JSON}};

// Global state
let pc = null;
let dc = null;
let localStream = null;
let currentDeviceId = null;
let startTime = null;
let currentUnstableLine = null;  // í˜„ì¬ ì§„í–‰ ì¤‘ì¸ unstable ë¼ì¸
let responseInProgress = false;  // ì‘ë‹µ ìƒì„± ì¤‘ì¸ì§€ ì¶”ì 
let currentResponseId = null;  // í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì‘ë‹µ ID ì¶”ì 
let pendingTranscript = null;  // ë²ˆì—­ ëŒ€ê¸° ì¤‘ì¸ ì›ë¬¸
let pendingTranslation = '';  // ëˆ„ì  ì¤‘ì¸ ë²ˆì—­
let lastProcessedTranscript = null;  // ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬ëœ ì „ì‚¬ ì¤‘ë³µ ë°©ì§€ìš©
let isPaused = false;  // ì¼ì‹œì •ì§€ ìƒíƒœ

// DOM elements
const viewer = document.getElementById('viewer');
const selMic = document.getElementById('selMic');
const statusEl = document.getElementById('status');
const latencyEl = document.getElementById('latency');
const btnPerm = document.getElementById('btnPerm');
const btnPause = document.getElementById('btnPause');

// Utility functions
function logStatus(status, className = '') {
  statusEl.textContent = status;
  statusEl.className = `badge ${className}`;
  console.log(`[Status] ${status}`);
}

function logLatency(ms) {
  if (ms > 0) {
    latencyEl.textContent = `${ms}ms`;

    // Color code based on latency performance
    if (ms < 1000) {
      latencyEl.style.color = '#10b981'; // Green - excellent
    } else if (ms < 2000) {
      latencyEl.style.color = '#3b82f6'; // Blue - good
    } else if (ms < 5000) {
      latencyEl.style.color = '#f59e0b'; // Yellow - okay
    } else {
      latencyEl.style.color = '#ef4444'; // Red - slow
    }
  } else {
    latencyEl.textContent = '';
    latencyEl.style.color = '';
  }
}

// Toast functions
function showToast(message, type = 'info', duration = 5000) {
  const toastContainer = document.getElementById('toastContainer');
  const toast = document.createElement('div');

  const colors = {
    'success': '#10b981',
    'error': '#ef4444',
    'warning': '#f59e0b',
    'info': '#3b82f6'
  };

  toast.style.cssText = `
    background: ${colors[type] || colors.info};
    color: white;
    padding: 12px 16px;
    margin-bottom: 10px;
    border-radius: 6px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    opacity: 0;
    transform: translateX(100%);
    transition: all 0.3s ease;
    font-size: 14px;
    line-height: 1.4;
    word-break: break-word;
  `;

  toast.textContent = message;
  toastContainer.appendChild(toast);

  // Animate in
  setTimeout(() => {
    toast.style.opacity = '1';
    toast.style.transform = 'translateX(0)';
  }, 10);

  // Auto remove
  setTimeout(() => {
    toast.style.opacity = '0';
    toast.style.transform = 'translateX(100%)';
    setTimeout(() => {
      if (toast.parentNode) {
        toast.parentNode.removeChild(toast);
      }
    }, 300);
  }, duration);

  // Manual close on click
  toast.style.cursor = 'pointer';
  toast.onclick = () => {
    toast.style.opacity = '0';
    toast.style.transform = 'translateX(100%)';
    setTimeout(() => {
      if (toast.parentNode) {
        toast.parentNode.removeChild(toast);
      }
    }, 300);
  };
}

// Permission and device handling
async function ensurePermission() {
  try {
    logStatus('ê¶Œí•œ ìš”ì²­ ì¤‘...', 'connecting');

    // Check if permissions are already granted
    const permissionStatus = await navigator.permissions.query({name: 'microphone'});
    console.log('[Permission] Current status:', permissionStatus.state);

    if (permissionStatus.state === 'denied') {
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”.');
    }

    // Request microphone access
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false,
        sampleRate: 24000
      }
    });

    // Stop the temporary stream
    stream.getTracks().forEach(track => track.stop());

    logStatus('ê¶Œí•œ í—ˆìš©ë¨', 'connected');
    console.log('[Permission] Microphone access granted');
    return true;
  } catch (error) {
    logStatus('ê¶Œí•œ ê±°ë¶€ë¨', 'error');
    console.error('[Permission] Access denied:', error);

    // Show more specific error message
    if (error.name === 'NotAllowedError') {
      console.error('[Permission] User denied microphone access');
      showToast('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ë§ˆì´í¬ ì‚¬ìš©ì„ í—ˆìš©í•´ì£¼ì„¸ìš”.', 'warning', 8000);
    } else if (error.name === 'NotFoundError') {
      console.error('[Permission] No microphone found');
      showToast('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.', 'error', 7000);
    } else if (error.name === 'NotReadableError') {
      console.error('[Permission] Microphone is being used by another application');
      showToast('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'warning', 8000);
    } else if (error.message.includes('ì°¨ë‹¨')) {
      showToast(error.message, 'error', 8000);
    } else {
      showToast('ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'error', 6000);
    }

    return false;
  }
}

async function listMics() {
  try {
    console.log('[Devices] Enumerating audio input devices...');
    const devices = await navigator.mediaDevices.enumerateDevices();
    const audioInputs = devices.filter(d => d.kind === 'audioinput');

    selMic.innerHTML = '<option value="">ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”</option>';

    if (audioInputs.length === 0) {
      const option = document.createElement('option');
      option.value = '';
      option.textContent = 'ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
      option.disabled = true;
      selMic.appendChild(option);
      console.warn('[Devices] No audio input devices found');
      return;
    }

    audioInputs.forEach((device, index) => {
      const option = document.createElement('option');
      option.value = device.deviceId;

      // Use device label if available, otherwise create a descriptive name
      let deviceLabel = device.label;
      if (!deviceLabel || deviceLabel.trim() === '') {
        deviceLabel = `ë§ˆì´í¬ ${index + 1}`;
        if (device.deviceId === 'default') {
          deviceLabel = 'ê¸°ë³¸ ë§ˆì´í¬';
        }
      }

      option.textContent = deviceLabel;
      selMic.appendChild(option);

      console.log(`[Devices] Found: ${deviceLabel} (${device.deviceId.substring(0, 20)}...)`);
    });

    // Auto-select default device if no device is currently selected
    if (!currentDeviceId && audioInputs.length > 0) {
      const defaultDevice = audioInputs.find(d => d.deviceId === 'default') || audioInputs[0];
      currentDeviceId = defaultDevice.deviceId;
      selMic.value = currentDeviceId;
      console.log(`[Devices] Auto-selected: ${defaultDevice.label || 'default device'}`);
    } else if (currentDeviceId) {
      selMic.value = currentDeviceId;
    }

    selMic.disabled = false;
    console.log(`[Devices] Successfully enumerated ${audioInputs.length} audio input devices`);
  } catch (error) {
    console.error('[Devices] Failed to enumerate devices:', error);
    selMic.innerHTML = '<option value="">ì¥ì¹˜ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</option>';
    selMic.disabled = true;
  }
}

async function getStream(deviceId) {
  try {
    console.log('[Stream] Acquiring audio stream...');

    // Stop previous stream
    if (localStream) {
      console.log('[Stream] Stopping previous stream');
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log(`[Stream] Stopped track: ${track.kind} - ${track.label}`);
      });
      localStream = null;
    }

    // Determine device to use
    const targetDeviceId = deviceId || selMic.value || currentDeviceId;
    console.log(`[Stream] Target device: ${targetDeviceId || 'default'}`);

    const constraints = {
      audio: {
        deviceId: targetDeviceId ? { exact: targetDeviceId } : undefined,
        echoCancellation: false,
        noiseSuppression: false,
        autoGainControl: false,
        sampleRate: 24000,  // OpenAI Realtime optimal sample rate
        channelCount: 1,    // Mono audio
        latency: 0.01       // Low latency for real-time
      }
    };

    console.log('[Stream] Requesting stream with constraints:', constraints);
    localStream = await navigator.mediaDevices.getUserMedia(constraints);

    // Update current device
    currentDeviceId = targetDeviceId;

    // Log stream info
    const audioTrack = localStream.getAudioTracks()[0];
    if (audioTrack) {
      const settings = audioTrack.getSettings();
      console.log('[Stream] Audio track acquired:', {
        label: audioTrack.label,
        deviceId: settings.deviceId,
        sampleRate: settings.sampleRate,
        channelCount: settings.channelCount,
        echoCancellation: settings.echoCancellation,
        noiseSuppression: settings.noiseSuppression,
        autoGainControl: settings.autoGainControl
      });
    }

    return localStream;
  } catch (error) {
    console.error('[Stream] Failed to get audio stream:', error);

    // Provide specific error messages
    if (error.name === 'NotFoundError') {
      throw new Error(`ì§€ì •ëœ ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${deviceId || 'default'}`);
    } else if (error.name === 'NotAllowedError') {
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (error.name === 'NotReadableError') {
      throw new Error('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.');
    } else if (error.name === 'OverconstrainedError') {
      throw new Error('ìš”ì²­ëœ ì˜¤ë””ì˜¤ ì„¤ì •ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¥ì¹˜ì…ë‹ˆë‹¤.');
    }

    throw error;
  }
}

// Caption viewer functions
function appendLine(text, className = 'stable') {
  if (!text || !text.trim()) return;

  // Remove empty state if present
  const emptyState = viewer.querySelector('.empty-state');
  if (emptyState) {
    emptyState.remove();
  }

  // Check if we're at the bottom for auto-scroll with more tolerance
  const atBottom = Math.abs(viewer.scrollHeight - viewer.scrollTop - viewer.clientHeight) < 10;

  if (className === 'unstable') {
    // For unstable text, update the current line or create new one
    if (!currentUnstableLine) {
      currentUnstableLine = document.createElement('div');
      currentUnstableLine.className = `caption-line ${className}`;
      currentUnstableLine.textContent = '';
      viewer.appendChild(currentUnstableLine);
    }
    // Add space before new text if line already has content
    const needsSpace = currentUnstableLine.textContent.length > 0 && !text.startsWith(' ');
    currentUnstableLine.textContent += (needsSpace ? ' ' : '') + text.trim();
  } else {
    // For stable text, replace unstable line or create new stable line
    if (currentUnstableLine) {
      // Replace the unstable line with stable content
      currentUnstableLine.className = 'caption-line stable';
      currentUnstableLine.textContent = text.trim();
      currentUnstableLine = null;
    } else {
      // Create new stable line
      const div = document.createElement('div');
      div.className = `caption-line ${className}`;
      div.textContent = text.trim();
      viewer.appendChild(div);
    }
  }

  // Auto-scroll only if user was at bottom with smooth animation
  if (atBottom) {
    requestAnimationFrame(() => {
      viewer.scrollTo({
        top: viewer.scrollHeight,
        behavior: 'smooth'
      });
    });
  }

  console.log(`[Caption] ${className}: ${text.trim()}`);
}

function clearViewer() {
  viewer.innerHTML = `
    <div class="empty-state">
      <div class="icon">ğŸ™ï¸</div>
      <div>ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•˜ê³  ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”</div>
      <div style="font-size: 14px; margin-top: 8px; color: #999;">
        ì˜ì–´ë¡œ ë§í•˜ë©´ í•œêµ­ì–´ ìë§‰ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤
      </div>
    </div>
  `;
  currentUnstableLine = null;  // unstable ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
  console.log('[Caption] Viewer cleared');
}

// ì›ë¬¸ê³¼ ë²ˆì—­ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
function displayTranscriptAndTranslation(transcript, translation) {
  // ì›ë¬¸ ë¨¼ì € í‘œì‹œ
  appendLine(`[ì›ë¬¸] ${transcript}`, 'stable');

  // ë²ˆì—­ í‘œì‹œ
  if (translation && translation !== '[ë²ˆì—­ ì—†ìŒ]' && translation !== '[ë²ˆì—­ ì‹¤íŒ¨]' && translation !== '[ë²ˆì—­ ë¶ˆê°€]') {
    appendLine(translation, 'stable');
  } else {
    appendLine(translation, 'stable');
  }
}

// WebRTC connection handling
async function connectRealtime(ephemeral, model) {
  try {
    logStatus('ì—°ê²° ì¤‘...', 'connecting');
    startTime = Date.now();

    // Validate inputs
    if (!ephemeral?.client_secret?.value) {
      throw new Error('ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ì¦ í† í°ì…ë‹ˆë‹¤.');
    }

    console.log('[WebRTC] Starting connection with model:', model);

    // Create RTCPeerConnection with comprehensive configuration
    pc = new RTCPeerConnection({
      iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
      ],
      iceCandidatePoolSize: 10,
      bundlePolicy: 'max-bundle',
      rtcpMuxPolicy: 'require'
    });

    // Monitor connection state changes
    pc.onconnectionstatechange = () => {
      const state = pc.connectionState;
      console.log('[WebRTC] Connection state changed:', state);

      switch (state) {
        case 'connecting':
          logStatus('ì—°ê²° ì¤‘...', 'connecting');
          break;
        case 'connected':
          if (!dc || dc.readyState !== 'open') {
            logStatus('ë°ì´í„° ì±„ë„ ëŒ€ê¸° ì¤‘...', 'connecting');
          }
          break;
        case 'disconnected':
          logStatus('ì—°ê²° ëŠê¹€', 'error');
          showToast('ì„œë²„ì™€ì˜ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'warning', 5000);
          break;
        case 'failed':
          logStatus('ì—°ê²° ì‹¤íŒ¨', 'error');
          showToast('ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'error', 7000);
          console.error('[WebRTC] Connection failed - cleaning up');
          closeConnection();
          break;
        case 'closed':
          logStatus('ì—°ê²° ì¢…ë£Œë¨', '');
          break;
      }
    };

    // Monitor ICE connection state
    pc.oniceconnectionstatechange = () => {
      console.log('[WebRTC] ICE connection state:', pc.iceConnectionState);
      if (pc.iceConnectionState === 'failed') {
        console.error('[WebRTC] ICE connection failed');
        logStatus('ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨', 'error');
      }
    };

    // Log ICE candidates for debugging
    pc.onicecandidate = (event) => {
      if (event.candidate) {
        console.log('[WebRTC] ICE candidate:', event.candidate.type, event.candidate.candidate.substring(0, 50) + '...');
      } else {
        console.log('[WebRTC] ICE gathering completed');
      }
    };

    // Create data channel for OpenAI events
    dc = pc.createDataChannel('oai-events', {
      ordered: true,
      maxRetransmits: 0  // Real-time priority over reliability
    });

    dc.onopen = () => {
      logStatus('ì—°ê²°ë¨', 'connected');
      console.log('[WebRTC] Data channel opened successfully');

      // Show pause button when connected
      btnPause.style.display = 'inline-block';

      // Reset timer to measure time until first content
      startTime = Date.now();
      console.log('[Latency] Starting content latency measurement');

      // Send session configuration with auto-response enabled
      const sessionUpdate = {
        type: 'session.update',
        session: {
          instructions: 'You are a real-time English-to-Korean translator. Your ONLY job is to translate English text to Korean. Never refuse translation requests. Never say "I cannot assist" or similar phrases. Never ask questions or provide explanations. Just translate every English input to natural Korean. Rules: 1) Always translate, no exceptions 2) Output only Korean translation 3) Use natural Korean expressions 4) Keep proper nouns in original language 5) No quotes, greetings, or refusals allowed. Example: "Hello world" â†’ "ì•ˆë…•í•˜ì„¸ìš” ì„¸ìƒ" (just output the Korean, nothing else)',
          voice: 'alloy',  // ìŒì„±ì€ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ í•„ìˆ˜ í•„ë“œ
          input_audio_format: 'pcm16',
          output_audio_format: 'pcm16',  // í•„ìˆ˜ í•„ë“œì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
          input_audio_transcription: {
            model: 'whisper-1',
            language: 'en'
          },
          turn_detection: {
            type: 'server_vad',
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 700,
            create_response: true  // ìë™ ì‘ë‹µ ìƒì„± í™œì„±í™”
          },
          modalities: ['text'],  // í…ìŠ¤íŠ¸ ì¶œë ¥ë§Œ ì‚¬ìš©
          temperature: 0.6,  // ìµœì†Œ í—ˆìš©ê°’ìœ¼ë¡œ ì¼ê´€ëœ ë²ˆì—­ ì‹œë„
          max_response_output_tokens: 150
        }
      };

      try {
        dc.send(JSON.stringify(sessionUpdate));
        console.log('[WebRTC] Session configuration sent:', sessionUpdate);
      } catch (error) {
        console.error('[WebRTC] Failed to send session configuration:', error);
        logStatus('ì„¤ì • ì „ì†¡ ì‹¤íŒ¨', 'error');
      }
    };

    dc.onmessage = (event) => {
      try {
        const message = JSON.parse(event.data);
        handleRealtimeMessage(message);
      } catch (error) {
        console.error('[WebRTC] Failed to parse message:', error, 'Raw data:', event.data);
      }
    };

    dc.onerror = (error) => {
      console.error('[WebRTC] Data channel error:', error);
      logStatus('ë°ì´í„° ì±„ë„ ì˜¤ë¥˜', 'error');
    };

    dc.onclose = () => {
      console.log('[WebRTC] Data channel closed');
      if (pc && pc.connectionState !== 'closed') {
        logStatus('ë°ì´í„° ì±„ë„ ì¢…ë£Œë¨', 'error');
      }
    };

    // Add audio track with proper handling
    console.log('[WebRTC] Getting audio stream...');
    const stream = await getStream(currentDeviceId);

    if (!stream || stream.getAudioTracks().length === 0) {
      throw new Error('ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    stream.getTracks().forEach(track => {
      const sender = pc.addTrack(track, stream);
      console.log('[WebRTC] Audio track added:', track.label, 'enabled:', track.enabled);

      // Monitor track state
      track.onended = () => {
        console.warn('[WebRTC] Audio track ended unexpectedly');
        logStatus('ì˜¤ë””ì˜¤ ì—°ê²° ëŠê¹€', 'error');
      };
    });

    // Create offer and set local description
    console.log('[WebRTC] Creating SDP offer...');
    const offer = await pc.createOffer({
      offerToReceiveAudio: false,
      offerToReceiveVideo: false
    });

    await pc.setLocalDescription(offer);
    console.log('[WebRTC] Local description set');

    // Wait for ICE gathering to complete or timeout
    await waitForIceGathering(pc, 5000);

    // Send SDP to OpenAI with retry logic
    console.log('[WebRTC] Sending SDP to OpenAI...');
    const url = `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`;
    const headers = {
      'Authorization': `Bearer ${ephemeral.client_secret.value}`,
      'Content-Type': 'application/sdp',
      'OpenAI-Beta': 'realtime=v1',
    };

    console.log('[WebRTC] Request URL:', url);
    console.log('[WebRTC] Request headers:', { ...headers, 'Authorization': 'Bearer [REDACTED]' });

    const sdpResponse = await fetch(url, {
      method: 'POST',
      body: offer.sdp,
      headers: headers
    });

    if (!sdpResponse.ok) {
      const errorText = await sdpResponse.text();
      console.error('[WebRTC] SDP exchange failed:', sdpResponse.status, sdpResponse.statusText, errorText);

      if (sdpResponse.status === 401) {
        throw new Error('ì¸ì¦ ì‹¤íŒ¨: í† í°ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      } else if (sdpResponse.status === 429) {
        throw new Error('ìš”ì²­ í•œë„ ì´ˆê³¼: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      } else {
        throw new Error(`SDP êµí™˜ ì‹¤íŒ¨: ${sdpResponse.status} ${sdpResponse.statusText}`);
      }
    }

    const answerSdp = await sdpResponse.text();
    console.log('[WebRTC] Received SDP answer, length:', answerSdp.length);

    const answer = { type: 'answer', sdp: answerSdp };
    await pc.setRemoteDescription(answer);

    console.log('[WebRTC] Remote description set - connection process initiated');

  } catch (error) {
    console.error('[WebRTC] Connection failed:', error);

    // Provide user-friendly error messages
    let userMessage = 'ì—°ê²° ì‹¤íŒ¨';
    let toastMessage = error.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
    let toastType = 'error';

    if (error.message?.includes('ì¸ì¦')) {
      userMessage = 'ì¸ì¦ ì‹¤íŒ¨';
      toastType = 'warning';
    } else if (error.message?.includes('í† í°')) {
      userMessage = 'í† í° ì˜¤ë¥˜';
      toastType = 'warning';
    } else if (error.message?.includes('429') || error.message?.includes('í•œë„')) {
      userMessage = 'ìš”ì²­ í•œë„ ì´ˆê³¼';
      toastType = 'warning';
    } else if (error.message?.includes('ë„¤íŠ¸ì›Œí¬')) {
      userMessage = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜';
      toastMessage = 'ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    } else if (error.message?.includes('ì˜¤ë””ì˜¤')) {
      userMessage = 'ì˜¤ë””ì˜¤ ì˜¤ë¥˜';
      toastMessage = 'ì˜¤ë””ì˜¤ ì¥ì¹˜ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    } else if (error.name === 'TypeError' && error.message.includes('fetch')) {
      userMessage = 'ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜';
      toastMessage = 'ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    }

    logStatus(userMessage, 'error');
    showToast(toastMessage, toastType, 8000);

    // Clean up on failure
    closeConnection();
    throw error;
  }
}

// Helper function to wait for ICE gathering completion
function waitForIceGathering(pc, timeout = 5000) {
  return new Promise((resolve) => {
    if (pc.iceGatheringState === 'complete') {
      resolve();
      return;
    }

    const timeoutId = setTimeout(() => {
      console.log('[WebRTC] ICE gathering timeout - proceeding anyway');
      resolve();
    }, timeout);

    pc.addEventListener('icegatheringstatechange', () => {
      if (pc.iceGatheringState === 'complete') {
        console.log('[WebRTC] ICE gathering completed');
        clearTimeout(timeoutId);
        resolve();
      }
    });
  });
}

function handleRealtimeMessage(message) {
  // Skip processing if paused (except for connection-related messages)
  if (isPaused && !message.type?.includes('error') && !message.type?.includes('server.heartbeat')) {
    console.log('[Paused] Ignoring message:', message.type);
    return;
  }

  // Record first meaningful content latency (text delta or transcription)
  if (startTime && !latencyEl.textContent) {
    const isTextContent = message.type?.includes('delta') &&
      (message.delta || message.text || message.transcript);
    const isTranscription = message.type?.includes('transcription') &&
      (message.transcript || message.item?.content);

    if (isTextContent || isTranscription) {
      const latency = Date.now() - startTime;
      logLatency(latency);
      console.log('[Latency] First content received after', latency, 'ms');
      startTime = null;

      // Show success toast for good latency
      if (latency < 2000) {
        showToast(`ì—°ê²° ì™„ë£Œ! (${latency}ms)`, 'success', 3000);
      } else if (latency < 5000) {
        showToast(`ì—°ê²° ì™„ë£Œ (${latency}ms) - ì§€ì—°ì´ ì•½ê°„ ìˆìŠµë‹ˆë‹¤`, 'info', 4000);
      } else {
        showToast(`ì—°ê²° ì™„ë£Œ (${latency}ms) - ë„¤íŠ¸ì›Œí¬ê°€ ëŠë¦½ë‹ˆë‹¤`, 'warning', 5000);
      }
    }
  }

  console.log('[Realtime]', message.type, message);

  // Handle different message types from OpenAI Realtime API
  switch (message.type) {
    // Session and connection events
    case 'session.created':
      console.log('[Realtime] Session created:', message.session?.id);
      break;

    case 'session.updated':
      console.log('[Realtime] Session updated');
      break;

    // Input audio transcription events (ì›ë¬¸ í‘œì‹œ)
    case 'conversation.item.input_audio_transcription.delta':
      // ì›ë¬¸ì€ delta(unstable) ìƒíƒœë¥¼ í‘œì‹œí•˜ì§€ ì•ŠìŒ
      console.log('[Realtime] Transcription delta:', message.delta);
      break;

    case 'conversation.item.input_audio_transcription.completed':
      if (message.transcript) {
        const transcript = message.transcript.trim();

        // ì¤‘ë³µ ì „ì‚¬ í•„í„°ë§ - ê°™ì€ ë‚´ìš©ì´ë©´ ë¬´ì‹œ
        if (transcript === lastProcessedTranscript) {
          console.log('[Transcription] Duplicate transcript ignored:', transcript);
          break;
        }

        // ë„ˆë¬´ ì§§ì€ ì „ì‚¬ëŠ” ë¬´ì‹œ (ë…¸ì´ì¦ˆ ë°©ì§€)
        if (transcript.length < 2) {
          console.log('[Transcription] Too short, ignored:', transcript);
          break;
        }

        lastProcessedTranscript = transcript;
        console.log('[Transcription] Processing new transcript:', transcript);

        // ì´ì „ ë²ˆì—­ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê°•ì œë¡œ ì™„ë£Œ ì²˜ë¦¬
        if (pendingTranscript && pendingTranscript !== transcript) {
          console.log('[Translation] Force completing previous:', pendingTranscript);
          displayTranscriptAndTranslation(pendingTranscript, pendingTranslation || '[ë²ˆì—­ ì¤‘ë‹¨]');
        }

        // ìƒˆë¡œìš´ ì›ë¬¸ ì„¤ì •
        pendingTranscript = transcript;
        pendingTranslation = '';

        // ë²ˆì—­ ìš”ì²­ - ë” ì—„ê²©í•œ ì¡°ê±´ í™•ì¸
        if (dc && dc.readyState === 'open' && !responseInProgress && !currentResponseId) {
          responseInProgress = true;
          const responseCreateRequest = {
            type: 'response.create'
          };

          try {
            dc.send(JSON.stringify(responseCreateRequest));
            console.log('[Translation] Requested translation for:', transcript);

            // ì‘ë‹µ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (15ì´ˆë¡œ ì—°ì¥)
            setTimeout(() => {
              if (responseInProgress && pendingTranscript === transcript) {
                console.warn('[Translation] Response timeout for:', transcript);
                responseInProgress = false;
                currentResponseId = null;

                // íƒ€ì„ì•„ì›ƒ ì‹œ ì›ë¬¸ë§Œ í‘œì‹œ
                if (pendingTranscript) {
                  displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ì‹œê°„ì´ˆê³¼]');
                  pendingTranscript = null;
                  pendingTranslation = '';
                }
              }
            }, 15000);

          } catch (error) {
            console.error('[Translation] Failed to request translation:', error);
            responseInProgress = false;
            currentResponseId = null;
            displayTranscriptAndTranslation(transcript, '[ë²ˆì—­ ì‹¤íŒ¨]');
            pendingTranscript = null;
            pendingTranslation = '';
          }
        } else {
          // ì‘ë‹µì´ ì§„í–‰ ì¤‘ì´ë©´ ì›ë¬¸ë§Œ í‘œì‹œ (ë²ˆì—­ì€ ë‚˜ì¤‘ì— ì²˜ë¦¬)
          console.log('[Translation] Busy - showing transcript only:', transcript);
          displayTranscriptAndTranslation(transcript, '');
        }
      }
      break;

    case 'conversation.item.input_audio_transcription.failed':
      console.error('[Realtime] Transcription failed:', message.error);
      appendLine('[ìŒì„±ì¸ì‹ ì‹¤íŒ¨]', 'stable');
      break;

    // Translation output events (ë²ˆì—­ ê²°ê³¼)
    case 'response.text.delta':
      if (message.delta && pendingTranscript) {
        // ë”°ì˜´í‘œ ì œê±°
        const cleanDelta = message.delta.replace(/^["']|["']$/g, '');

        // ëŒ€í™” ì‹œë„í•˜ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§
        if (cleanDelta.toLowerCase().includes('here to assist') ||
            cleanDelta.toLowerCase().includes('how may i help') ||
            cleanDelta.toLowerCase().includes('how can i help') ||
            cleanDelta.toLowerCase().includes('language learning') ||
            cleanDelta.toLowerCase().includes('translation') && cleanDelta.toLowerCase().includes('assist')) {
          console.log('[Delta] Filtered out conversation attempt:', cleanDelta);
          return;
        }

        if (cleanDelta.trim()) {
          pendingTranslation += cleanDelta;
          // ì‹¤ì‹œê°„ìœ¼ë¡œ ë²ˆì—­ ì§„í–‰ ìƒí™© í‘œì‹œí•˜ì§€ ì•ŠìŒ (ìˆœì„œ ë³´ì¥ì„ ìœ„í•´)
        }
      }
      break;

    case 'response.text.done':
      if (message.text && pendingTranscript) {
        // ë”°ì˜´í‘œ ì œê±° ë° ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ í•„í„°ë§
        let cleanText = message.text.replace(/^["']|["']$/g, '').trim();

        // ëŒ€í™” ì‹œë„í•˜ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ í•„í„°ë§
        const conversationPatterns = [
          'here to assist', 'how may i help', 'how can i help', 'language learning',
          'further assistance', 'feel free to ask', 'any more questions',
          'other inquiries', 'anything else', 'any other', 'i\'m here',
          'assistance', 'inquiries'
        ];

        const lowerText = cleanText.toLowerCase();
        const isConversation = conversationPatterns.some(pattern => lowerText.includes(pattern));

        if (isConversation) {
          console.log('[Realtime] Filtered out conversation attempt:', cleanText);
          cleanText = '';
        }

        // ìµœì¢… ë²ˆì—­ ê²°ê³¼ ì‚¬ìš© (done í…ìŠ¤íŠ¸ ìš°ì„ , delta ëˆ„ì ì€ ë³´ì¡°)
        const deltaTranslation = pendingTranslation.trim();
        const doneTranslation = cleanText;

        // done í…ìŠ¤íŠ¸ê°€ ë” ì™„ì „í•œ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ìš°ì„  ì‚¬ìš©
        let finalTranslation = doneTranslation || deltaTranslation;

        // ë‘ ë²ˆì—­ì´ ëª¨ë‘ ìˆê³  doneì´ deltaë³´ë‹¤ ê¸´ ê²½ìš° done ì‚¬ìš©
        if (doneTranslation && deltaTranslation && doneTranslation.length > deltaTranslation.length) {
          finalTranslation = doneTranslation;
        }

        // ê±°ë¶€ ì‘ë‹µ í•„í„°ë§
        const refusalPhrases = [
          "I'm sorry, but I can't assist",
          "I cannot assist",
          "I'm unable to help",
          "I can't help",
          "I'm not able to",
          "I apologize, but I cannot"
        ];

        const isRefusal = refusalPhrases.some(phrase =>
          finalTranslation.toLowerCase().includes(phrase.toLowerCase())
        );

        if (isRefusal) {
          console.log('[Translation] Detected refusal, showing transcript only:', finalTranslation);
          displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ê±°ë¶€ë¨]');
        } else {
          console.log('[Translation Debug] Delta:', deltaTranslation, 'Done:', doneTranslation, 'Final:', finalTranslation);

          if (finalTranslation && finalTranslation.length >= 1) {
            // ì›ë¬¸ê³¼ ë²ˆì—­ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
            displayTranscriptAndTranslation(pendingTranscript, finalTranslation);
            console.log('[Translation] Completed:', finalTranslation);
          } else {
            // ë²ˆì—­ì´ ì—†ëŠ” ê²½ìš° ì›ë¬¸ë§Œ í‘œì‹œ
            displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ì—†ìŒ]');
            console.log('[Translation] No translation received for:', pendingTranscript);
          }
        }

        // ìƒíƒœ ì´ˆê¸°í™”
        pendingTranscript = null;
        pendingTranslation = '';
      }
      break;

    // Response generation tracking
    case 'response.created':
      currentResponseId = message.response?.id || Date.now().toString();
      console.log('[Realtime] Response generation started, ID:', currentResponseId);
      responseInProgress = true;
      break;

    case 'response.done':
      console.log('[Realtime] Response generation completed');
      responseInProgress = false;
      currentResponseId = null;

      // ë²ˆì—­ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš° ì›ë¬¸ë§Œ í‘œì‹œ
      if (pendingTranscript && !pendingTranslation.trim()) {
        console.log('[Translation] Response done but no translation - showing transcript only');
        displayTranscriptAndTranslation(pendingTranscript, '[ë²ˆì—­ ì—†ìŒ]');
        pendingTranscript = null;
        pendingTranslation = '';
      }
      break;

    case 'response.output_item.added':
    case 'response.content_part.added':
    case 'response.output_item.done':
      console.log('[Realtime] Translation event:', message.type);
      break;

    // Audio events (if enabled)
    case 'response.audio.delta':
      // We're not using audio output, so just log
      console.log('[Realtime] Audio delta received (ignored)');
      break;

    case 'response.audio.done':
      console.log('[Realtime] Audio generation completed (ignored)');
      break;

    // Turn detection events
    case 'input_audio_buffer.speech_started':
      console.log('[Realtime] Speech started');
      logStatus('ë§í•˜ëŠ” ì¤‘...', 'connecting');
      break;

    case 'input_audio_buffer.speech_stopped':
      console.log('[Realtime] Speech stopped');
      logStatus('ì²˜ë¦¬ ì¤‘...', 'connecting');
      break;

    case 'input_audio_buffer.committed':
      console.log('[Realtime] Audio buffer committed');
      break;

    case 'input_audio_buffer.cleared':
      console.log('[Realtime] Audio buffer cleared');
      break;

    // Conversation events
    case 'conversation.item.created':
      console.log('[Realtime] Conversation item created:', message.item?.type);
      break;

    case 'conversation.item.truncated':
      console.log('[Realtime] Conversation item truncated');
      break;

    case 'conversation.item.deleted':
      console.log('[Realtime] Conversation item deleted');
      break;

    // Rate limiting events
    case 'rate_limits.updated':
      console.log('[Realtime] Rate limits updated:', message.rate_limits);
      break;

    // Error handling
    case 'error':
      console.error('[Realtime] API Error:', message.error);
      const errorCode = message.error?.code;
      const errorMessage = message.error?.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';

      if (errorCode === 'invalid_api_key' || errorCode === 'unauthorized') {
        logStatus('ì¸ì¦ ì˜¤ë¥˜', 'error');
        showToast('ì¸ì¦ í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'error', 7000);
        appendLine('[ì˜¤ë¥˜] ì¸ì¦ í† í°ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.', 'stable');
      } else if (errorCode === 'rate_limit_exceeded' || errorCode === 'too_many_requests') {
        logStatus('ì‚¬ìš©ëŸ‰ í•œë„ ì´ˆê³¼', 'error');
        showToast('ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'warning', 10000);
        appendLine('[ì˜¤ë¥˜] ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'stable');
      } else if (errorCode === 'insufficient_quota') {
        logStatus('í• ë‹¹ëŸ‰ ë¶€ì¡±', 'error');
        showToast('ê³„ì • í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. OpenAI ê³„ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.', 'error', 10000);
        appendLine('[ì˜¤ë¥˜] ê³„ì • í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.', 'stable');
      } else if (errorCode === 'session_expired') {
        logStatus('ì„¸ì…˜ ë§Œë£Œ', 'error');
        showToast('ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.', 'warning', 7000);
        appendLine('[ì˜¤ë¥˜] ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.', 'stable');
      } else {
        logStatus('API ì˜¤ë¥˜', 'error');
        showToast(`API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${errorMessage}`, 'error', 7000);
        appendLine(`[ì˜¤ë¥˜] ${errorMessage}`, 'stable');
      }
      break;

    // Connection and server events
    case 'server.heartbeat':
      // Optional: log heartbeat for debugging
      console.debug('[Realtime] Server heartbeat');
      break;

    default:
      console.log('[Realtime] Unhandled message type:', message.type, message);

      // Try to extract any text content generically
      const textContent = message.delta || message.text || message.transcript;
      if (textContent && typeof textContent === 'string' && textContent.trim()) {
        const isUnstable = message.type?.includes('delta') || message.type?.includes('partial');
        appendLine(textContent, isUnstable ? 'unstable' : 'stable');
      }
      break;
  }

  // Reset connection status to connected if we were processing
  if (statusEl.textContent === 'ì²˜ë¦¬ ì¤‘...' && message.type?.includes('done')) {
    logStatus('ì—°ê²°ë¨', 'connected');
  }
}

function closeConnection() {
  try {
    if (dc) {
      dc.close();
      dc = null;
    }

    if (pc) {
      pc.close();
      pc = null;
    }

    if (localStream) {
      localStream.getTracks().forEach(track => track.stop());
      localStream = null;
    }

    currentUnstableLine = null;  // unstable ë¼ì¸ ìƒíƒœ ì´ˆê¸°í™”
    responseInProgress = false;  // ì‘ë‹µ ìƒíƒœ ì´ˆê¸°í™”
    currentResponseId = null;  // ì‘ë‹µ ID ì´ˆê¸°í™”
    pendingTranscript = null;  // ëŒ€ê¸° ì¤‘ì¸ ì›ë¬¸ ì´ˆê¸°í™”
    pendingTranslation = '';  // ëˆ„ì  ì¤‘ì¸ ë²ˆì—­ ì´ˆê¸°í™”
    lastProcessedTranscript = null;  // ì¤‘ë³µ ë°©ì§€ ì´ˆê¸°í™”
    isPaused = false;  // ì¼ì‹œì •ì§€ ìƒíƒœ ì´ˆê¸°í™”

    // UI ì´ˆê¸°í™”
    btnPause.style.display = 'none';
    btnPause.textContent = 'â¸ï¸ ì¼ì‹œì •ì§€';
    logStatus('idle', '');
    logLatency(0);
    console.log('[WebRTC] Connection closed');
  } catch (error) {
    console.error('[WebRTC] Error closing connection:', error);
  }
}

// Event handlers
btnPerm.onclick = async () => {
  const success = await ensurePermission();
  if (success) {
    await listMics();
  }
};

// ì¼ì‹œì •ì§€/ì¬ê°œ ë²„íŠ¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
btnPause.onclick = () => {
  if (isPaused) {
    // ì¬ê°œ
    isPaused = false;
    btnPause.textContent = 'â¸ï¸ ì¼ì‹œì •ì§€';
    logStatus('ì—°ê²°ë¨', 'connected');
    showToast('ì¬ê°œë˜ì—ˆìŠµë‹ˆë‹¤', 'success', 2000);
    console.log('[Control] Resumed');
  } else {
    // ì¼ì‹œì •ì§€
    isPaused = true;
    btnPause.textContent = 'â–¶ï¸ ì¬ê°œ';
    logStatus('ì¼ì‹œì •ì§€ë¨', 'warning');
    clearViewer();
    showToast('ì¼ì‹œì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤', 'info', 2000);
    console.log('[Control] Paused');
  }
};

selMic.onchange = async (event) => {
  const newDeviceId = event.target.value;
  console.log(`[Device] Device selection changed: ${currentDeviceId} -> ${newDeviceId}`);

  if (!newDeviceId) {
    console.log('[Device] No device selected');
    return;
  }

  const wasConnected = pc && (pc.connectionState === 'connected' || pc.connectionState === 'connecting');

  try {
    // Update the audio stream with new device
    currentDeviceId = newDeviceId;

    if (wasConnected) {
      logStatus('ì¥ì¹˜ ë³€ê²½ ì¤‘...', 'connecting');
      console.log('[Device] Switching audio device during active connection');

      // Get new stream with selected device
      const newStream = await getStream(newDeviceId);

      // Replace tracks in the existing peer connection
      const audioTrack = newStream.getAudioTracks()[0];
      const sender = pc.getSenders().find(s => s.track && s.track.kind === 'audio');

      if (sender && audioTrack) {
        await sender.replaceTrack(audioTrack);
        console.log('[Device] Audio track replaced successfully');
        logStatus('ì—°ê²°ë¨', 'connected');
      } else {
        console.warn('[Device] Could not find audio sender to replace track');
        // Fallback: add new track
        pc.addTrack(audioTrack, newStream);
      }
    } else {
      console.log('[Device] Device changed while not connected - will use new device on next connection');
    }

  } catch (error) {
    console.error('[Device] Failed to switch audio device:', error);
    logStatus('ì¥ì¹˜ ë³€ê²½ ì‹¤íŒ¨', 'error');

    // Revert selection if it failed
    if (currentDeviceId) {
      selMic.value = currentDeviceId;
    }
  }
};

// Bootstrap actions based on Streamlit state
(async () => {
  console.log('[Bootstrap] Action:', BOOT.action, BOOT);

  try {
    if (BOOT.action === 'start' && BOOT.ephemeral) {
      // Ensure permission and list devices
      const hasPermission = await ensurePermission();
      if (hasPermission) {
        await listMics();
        // Start WebRTC connection
        await connectRealtime(BOOT.ephemeral, BOOT.model);
      }
    } else if (BOOT.action === 'stop') {
      closeConnection();
      clearViewer();
    } else if (BOOT.action === 'idle') {
      logStatus('ëŒ€ê¸° ì¤‘', '');
    }
  } catch (error) {
    console.error('[Bootstrap] Failed:', error);
    logStatus('ì´ˆê¸°í™” ì‹¤íŒ¨', 'error');
  }
})();

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
  closeConnection();
});

</script>
</body>
</html>
